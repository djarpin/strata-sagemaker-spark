{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker PySpark XGBoost MNIST Example (Spark EMR Cluster)\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Setup](#Setup)\n",
    "3. [Loading the Data](#Loading-the-Data)\n",
    "4. [Training and Hosting a Model](#Training-and-Hosting-a-Model)\n",
    "5. [Inference](#Inference)\n",
    "6. [More on SageMaker Spark](#More-on-SageMaker-Spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This notebook will show how to classify handwritten digits using the XGBoost algorithm on Amazon SageMaker through the SageMaker PySpark library. We will train on Amazon SageMaker using XGBoost on the MNIST dataset, host the trained model on Amazon SageMaker, and then make predictions against that hosted model.\n",
    "\n",
    "Unlike the other notebooks that demonstrate XGBoost on Amazon SageMaker, this notebook uses a SparkSession to manipulate data, and uses the SageMaker Spark library to interact with SageMaker with Spark Estimators and Transformers.\n",
    "\n",
    "You can visit SageMaker Spark's GitHub repository at https://github.com/aws/sagemaker-spark to learn more about SageMaker Spark.\n",
    "\n",
    "You can visit XGBoost's GitHub repository at https://github.com/dmlc/xgboost to learn more about XGBoost\n",
    "\n",
    "This notebook was created and tested on an ml.m4.xlarge notebook instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, we import the necessary modules and create the SparkSession with the SageMaker Spark dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import sagemaker_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'executorMemory': '2g', 'executorCores': 2, 'numExecutors': 4, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1536534062046_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-11-101.us-west-2.compute.internal:20888/proxy/application_1536534062046_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-10-186.us-west-2.compute.internal:8042/node/containerlogs/container_1536534062046_0001_01_000001/livy\">Link</a></td><td></td></tr><tr><td>1</td><td>application_1536534062046_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-11-101.us-west-2.compute.internal:20888/proxy/application_1536534062046_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-10-186.us-west-2.compute.internal:8042/node/containerlogs/container_1536534062046_0002_01_000001/livy\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, we're running in a different kernel and we can see details on our Spark EMR cluster and session.\n",
    "\n",
    "## Loading the Data\n",
    "\n",
    "Now, we load the MNIST dataset into a Spark Dataframe, which dataset is available in LibSVM format at\n",
    "\n",
    "`s3://sagemaker-sample-data-[region]/spark/mnist/train/`\n",
    "\n",
    "where `[region]` is replaced with a supported AWS region, such as us-east-1.\n",
    "\n",
    "In order to train and make inferences our input DataFrame must have a column of Doubles (named \"label\" by default) and a column of Vectors of Doubles (named \"features\" by default).\n",
    "\n",
    "Spark's LibSVM DataFrameReader loads a DataFrame already suitable for training and inference.\n",
    "\n",
    "Here, we load into a DataFrame in the SparkSession running on the local Notebook Instance, but you can connect your Notebook Instance to a remote Spark cluster for heavier workloads. Starting from EMR 5.11.0, SageMaker Spark is pre-installed on EMR Spark clusters. For more on connecting your SageMaker Notebook Instance to a remote EMR cluster, please see [this blog post](https://aws.amazon.com/blogs/machine-learning/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  5.0|[0.0,0.0,0.0,0.0,...|\n",
      "|  0.0|[0.0,0.0,0.0,0.0,...|\n",
      "|  4.0|[0.0,0.0,0.0,0.0,...|\n",
      "|  1.0|[0.0,0.0,0.0,0.0,...|\n",
      "|  9.0|[0.0,0.0,0.0,0.0,...|\n",
      "|  2.0|[0.0,0.0,0.0,0.0,...|\n",
      "|  1.0|[0.0,0.0,0.0,0.0,...|\n",
      "|  3.0|[0.0,0.0,0.0,0.0,...|\n",
      "|  1.0|[0.0,0.0,0.0,0.0,...|\n",
      "|  4.0|[0.0,0.0,0.0,0.0,...|\n",
      "|  3.0|[0.0,0.0,0.0,0.0,...|\n",
      "|  5.0|[0.0,0.0,0.0,0.0,...|\n",
      "|  3.0|[0.0,0.0,0.0,0.0,...|\n",
      "|  6.0|[0.0,0.0,0.0,0.0,...|\n",
      "|  1.0|[0.0,0.0,0.0,0.0,...|\n",
      "|  7.0|[0.0,0.0,0.0,0.0,...|\n",
      "|  2.0|[0.0,0.0,0.0,0.0,...|\n",
      "|  8.0|[0.0,0.0,0.0,0.0,...|\n",
      "|  6.0|[0.0,0.0,0.0,0.0,...|\n",
      "|  9.0|[0.0,0.0,0.0,0.0,...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "region = '<your_aws_region_here>'\n",
    "\n",
    "trainingData = spark.read.format('libsvm')\\\n",
    "    .option('numFeatures', '784')\\\n",
    "    .option('vectorType', 'dense')\\\n",
    "    .load('s3a://sagemaker-sample-data-{}/spark/mnist/train/'.format(region))\n",
    "\n",
    "testData = spark.read.format('libsvm')\\\n",
    "    .option('numFeatures', '784')\\\n",
    "    .option('vectorType', 'dense')\\\n",
    "    .load('s3a://sagemaker-sample-data-{}/spark/mnist/test/'.format(region))\n",
    "\n",
    "trainingData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Hosting a Model\n",
    "Now we create an XGBoostSageMakerEstimator, which uses the XGBoost Amazon SageMaker Algorithm to train on our input data, and uses the XGBoost Amazon SageMaker model image to host our model.\n",
    "\n",
    "Calling fit() on this estimator will train our model on Amazon SageMaker, and then create an Amazon SageMaker Endpoint to host our model.\n",
    "\n",
    "We can then use the SageMakerModel returned by this call to fit() to transform Dataframes using our hosted model.\n",
    "\n",
    "The following cell runs a training job and creates an endpoint to host the resulting model, so this cell can take up to twenty minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sagemaker_pyspark import IAMRole, S3DataPath\n",
    "from sagemaker_pyspark.algorithms import XGBoostSageMakerEstimator\n",
    "\n",
    "xgboost_estimator = XGBoostSageMakerEstimator(\n",
    "    sagemakerRole=IAMRole('your_sagemaker_iam_role_arn_here'),\n",
    "    trainingInstanceType='ml.m4.xlarge',\n",
    "    trainingInstanceCount=1,\n",
    "    endpointInstanceType='ml.m4.xlarge',\n",
    "    endpointInitialInstanceCount=1)\n",
    "\n",
    "xgboost_estimator.setEta(0.2)\n",
    "xgboost_estimator.setGamma(4)\n",
    "xgboost_estimator.setMinChildWeight(6)\n",
    "xgboost_estimator.setSilent(0)\n",
    "xgboost_estimator.setObjective(\"multi:softmax\")\n",
    "xgboost_estimator.setNumClasses(10)\n",
    "xgboost_estimator.setNumRound(10)\n",
    "\n",
    "# train\n",
    "model = xgboost_estimator.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "Now we transform our DataFrame.\n",
    "To do this, we serialize each row's \"features\" Vector of Doubles into LibSVM format for inference against the Amazon SageMaker Endpoint. We deserialize the CSV responses from the XGBoost model back into our DataFrame. This serialization and deserialization is handled automatically by the `transform()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----------+\n",
      "|label|            features|prediction|\n",
      "+-----+--------------------+----------+\n",
      "|  5.0|[0.0,0.0,0.0,0.0,...|       5.0|\n",
      "|  0.0|[0.0,0.0,0.0,0.0,...|       0.0|\n",
      "|  4.0|[0.0,0.0,0.0,0.0,...|       4.0|\n",
      "|  1.0|[0.0,0.0,0.0,0.0,...|       1.0|\n",
      "|  9.0|[0.0,0.0,0.0,0.0,...|       9.0|\n",
      "|  2.0|[0.0,0.0,0.0,0.0,...|       2.0|\n",
      "|  1.0|[0.0,0.0,0.0,0.0,...|       1.0|\n",
      "|  3.0|[0.0,0.0,0.0,0.0,...|       3.0|\n",
      "|  1.0|[0.0,0.0,0.0,0.0,...|       1.0|\n",
      "|  4.0|[0.0,0.0,0.0,0.0,...|       4.0|\n",
      "|  3.0|[0.0,0.0,0.0,0.0,...|       3.0|\n",
      "|  5.0|[0.0,0.0,0.0,0.0,...|       5.0|\n",
      "|  3.0|[0.0,0.0,0.0,0.0,...|       3.0|\n",
      "|  6.0|[0.0,0.0,0.0,0.0,...|       6.0|\n",
      "|  1.0|[0.0,0.0,0.0,0.0,...|       1.0|\n",
      "|  7.0|[0.0,0.0,0.0,0.0,...|       7.0|\n",
      "|  2.0|[0.0,0.0,0.0,0.0,...|       2.0|\n",
      "|  8.0|[0.0,0.0,0.0,0.0,...|       8.0|\n",
      "|  6.0|[0.0,0.0,0.0,0.0,...|       6.0|\n",
      "|  9.0|[0.0,0.0,0.0,0.0,...|       9.0|\n",
      "+-----+--------------------+----------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "transformedData = model.transform(trainingData)\n",
    "\n",
    "transformedData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we don't need to make any more inferences, now we delete the endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the endpoint\n",
    "\n",
    "from sagemaker_pyspark import SageMakerResourceCleanup\n",
    "\n",
    "resource_cleanup = SageMakerResourceCleanup(model.sagemakerClient)\n",
    "resource_cleanup.deleteResources(model.getCreatedResources())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on SageMaker Spark\n",
    "\n",
    "The SageMaker Spark Github repository has more about SageMaker Spark, including how to use SageMaker Spark with your own algorithms on Amazon SageMaker: https://github.com/aws/sagemaker-spark\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
